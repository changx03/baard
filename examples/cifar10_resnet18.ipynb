{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation((-7, 7)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_train = tv.datasets.CIFAR10(DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "dataset_test = tv.datasets.CIFAR10(DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet18(nn.Module):\n",
    "#     def __init__(self, n_labels) -> None:\n",
    "#         super(ResNet18, self).__init__()\n",
    "#         self.n_labels = n_labels\n",
    "\n",
    "#         # Load pre-trained resnet model\n",
    "#         resnet = tv.models.resnet18(weights=tv.models.ResNet18_Weights.IMAGENET1K_V1, progress=False)\n",
    "\n",
    "#         self.conv1 = resnet.conv1\n",
    "#         self.bn1 = resnet.bn1\n",
    "#         self.relu = resnet.relu\n",
    "#         self.maxpool = resnet.maxpool\n",
    "#         self.layer1 = resnet.layer1\n",
    "#         self.layer2 = resnet.layer2\n",
    "#         self.layer3 = resnet.layer3\n",
    "#         self.layer4 = resnet.layer4\n",
    "#         self.avgpool = resnet.avgpool\n",
    "#         n_in_features = resnet.fc.in_features\n",
    "#         self.fc = nn.Linear(n_in_features, self.n_labels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = iter(dataset_train)\n",
    "# x, y = next(i)\n",
    "# input_size = tuple([BATCH_SIZE] + list(x.size()))\n",
    "# print('input_size:', input_size)\n",
    "\n",
    "# summary(model=ResNet18(10), input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tv.models.resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "# summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer, device):\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    running_loss = 0.\n",
    "    corrects = 0.\n",
    "\n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    acc = corrects / n\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    running_loss = 0.\n",
    "    corrects = 0.\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x= x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            \n",
    "            preds = outputs.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / n_batches\n",
    "    acc = corrects / n\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            # print(\n",
    "            #     f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                # print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.05\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTORCH LIGHTNING CIFAR10 ~94% BASELINE TUTORIA [URL](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/cifar10-baseline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "steps_per_epoch = len(loader_train)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 0.1, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=EPOCHS):\n",
    "    early_stopping = EarlyStopping()\n",
    "\n",
    "    time_start = time.perf_counter()\n",
    "    for e in range(epochs):\n",
    "        epoch_start = time.perf_counter()\n",
    "        tr_loss, tr_acc = train(model, loader_train, loss, optimizer, device)\n",
    "        va_loss, va_acc = evaluate(model, loader_test, loss, device)\n",
    "        scheduler.step()\n",
    "        epoch_end = time.perf_counter() - epoch_start\n",
    "\n",
    "        print('[{:3d}/{:d} T:{:s}] Train Loss: {:.4f} Acc: {:.4f}%, Test Loss: {:.4f} Acc: {:.4f}%'.format(\n",
    "            e+1, epochs, str(datetime.timedelta(seconds=epoch_end)), tr_loss, tr_acc*100, va_loss, va_acc*100))\n",
    "\n",
    "        early_stopping(tr_loss)\n",
    "        if early_stopping.early_stop:\n",
    "                break\n",
    "        \n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        test_losses.append(va_loss)\n",
    "        test_accs.append(va_acc)\n",
    "    time_elapsed = time.perf_counter()\n",
    "    print('Total training time: {}'.format(str(datetime.timedelta(seconds=time_elapsed))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/50 T:0:00:36.450871] Train Loss: 1.7544 Acc: 34.1340%, Test Loss: 1.5617 Acc: 42.3500%\n",
      "[  2/50 T:0:00:35.660056] Train Loss: 1.3262 Acc: 51.5280%, Test Loss: 1.2019 Acc: 56.2400%\n",
      "[  3/50 T:0:00:34.768923] Train Loss: 1.1093 Acc: 59.9660%, Test Loss: 1.0699 Acc: 62.9000%\n",
      "[  4/50 T:0:00:35.035917] Train Loss: 0.9778 Acc: 65.1060%, Test Loss: 1.0295 Acc: 64.2700%\n",
      "[  5/50 T:0:00:34.952071] Train Loss: 0.8822 Acc: 68.7700%, Test Loss: 0.9051 Acc: 68.1500%\n",
      "[  6/50 T:0:00:34.731951] Train Loss: 0.8114 Acc: 71.0500%, Test Loss: 0.8415 Acc: 70.8000%\n",
      "[  7/50 T:0:00:35.513442] Train Loss: 0.7361 Acc: 73.8560%, Test Loss: 0.6907 Acc: 75.7700%\n",
      "[  8/50 T:0:00:35.180229] Train Loss: 0.6729 Acc: 76.3320%, Test Loss: 0.6986 Acc: 76.2700%\n",
      "[  9/50 T:0:00:34.916122] Train Loss: 0.6347 Acc: 77.5920%, Test Loss: 0.7625 Acc: 74.7000%\n",
      "[ 10/50 T:0:00:35.761907] Train Loss: 0.5841 Acc: 79.5480%, Test Loss: 0.6076 Acc: 79.6600%\n",
      "[ 11/50 T:0:00:35.134597] Train Loss: 0.5427 Acc: 80.8960%, Test Loss: 0.5851 Acc: 80.1100%\n",
      "[ 12/50 T:0:00:35.362251] Train Loss: 0.5175 Acc: 81.8660%, Test Loss: 0.6675 Acc: 78.8300%\n",
      "[ 13/50 T:0:00:35.450900] Train Loss: 0.4943 Acc: 82.7140%, Test Loss: 0.6077 Acc: 80.1200%\n",
      "[ 14/50 T:0:00:35.405994] Train Loss: 0.4699 Acc: 83.4880%, Test Loss: 0.5623 Acc: 81.4200%\n",
      "[ 15/50 T:0:00:35.617625] Train Loss: 0.4420 Acc: 84.3960%, Test Loss: 0.5560 Acc: 82.0200%\n",
      "[ 16/50 T:0:00:34.729935] Train Loss: 0.4248 Acc: 85.0160%, Test Loss: 0.5699 Acc: 81.5600%\n",
      "[ 17/50 T:0:00:35.083668] Train Loss: 0.4062 Acc: 85.6400%, Test Loss: 0.5255 Acc: 82.4000%\n",
      "[ 18/50 T:0:00:33.284633] Train Loss: 0.3830 Acc: 86.5940%, Test Loss: 0.5079 Acc: 83.8600%\n",
      "[ 19/50 T:0:00:31.924206] Train Loss: 0.3718 Acc: 86.9880%, Test Loss: 0.5160 Acc: 84.1800%\n",
      "[ 20/50 T:0:00:35.213549] Train Loss: 0.3606 Acc: 87.2520%, Test Loss: 0.5082 Acc: 84.4000%\n",
      "[ 21/50 T:0:00:36.452930] Train Loss: 0.3396 Acc: 88.0300%, Test Loss: 0.4807 Acc: 84.5500%\n",
      "[ 22/50 T:0:00:37.207354] Train Loss: 0.3322 Acc: 88.3660%, Test Loss: 0.5118 Acc: 84.7300%\n",
      "[ 23/50 T:0:00:37.495182] Train Loss: 0.3153 Acc: 89.1200%, Test Loss: 0.4974 Acc: 84.1500%\n",
      "[ 24/50 T:0:00:37.224303] Train Loss: 0.3024 Acc: 89.3760%, Test Loss: 0.4727 Acc: 85.2200%\n",
      "[ 25/50 T:0:00:35.384992] Train Loss: 0.2838 Acc: 90.0200%, Test Loss: 0.4210 Acc: 86.4300%\n",
      "[ 26/50 T:0:00:35.552491] Train Loss: 0.2800 Acc: 90.1500%, Test Loss: 0.4638 Acc: 85.4600%\n",
      "[ 27/50 T:0:00:35.188685] Train Loss: 0.2715 Acc: 90.3940%, Test Loss: 0.4472 Acc: 86.5900%\n",
      "[ 28/50 T:0:00:35.636811] Train Loss: 0.2597 Acc: 90.9800%, Test Loss: 0.4159 Acc: 86.2800%\n",
      "[ 29/50 T:0:00:35.551359] Train Loss: 0.2493 Acc: 91.2420%, Test Loss: 0.4567 Acc: 86.4900%\n",
      "[ 30/50 T:0:00:36.321802] Train Loss: 0.2468 Acc: 91.3000%, Test Loss: 0.4266 Acc: 87.2600%\n",
      "[ 31/50 T:0:00:36.745793] Train Loss: 0.2331 Acc: 91.8700%, Test Loss: 0.4762 Acc: 85.4100%\n",
      "[ 32/50 T:0:00:36.403487] Train Loss: 0.2316 Acc: 91.7660%, Test Loss: 0.4387 Acc: 86.7100%\n",
      "[ 33/50 T:0:00:36.291765] Train Loss: 0.2211 Acc: 92.1740%, Test Loss: 0.4592 Acc: 86.0600%\n",
      "[ 34/50 T:0:00:36.440745] Train Loss: 0.2126 Acc: 92.4040%, Test Loss: 0.3988 Acc: 87.5900%\n",
      "[ 35/50 T:0:00:36.272822] Train Loss: 0.2030 Acc: 92.9480%, Test Loss: 0.4392 Acc: 86.8000%\n",
      "[ 36/50 T:0:00:36.533291] Train Loss: 0.2032 Acc: 92.8260%, Test Loss: 0.4123 Acc: 88.2400%\n",
      "[ 37/50 T:0:00:36.775419] Train Loss: 0.1859 Acc: 93.5040%, Test Loss: 0.4169 Acc: 87.4800%\n",
      "[ 38/50 T:0:00:36.769329] Train Loss: 0.1831 Acc: 93.4360%, Test Loss: 0.3961 Acc: 88.2900%\n",
      "[ 39/50 T:0:00:35.993805] Train Loss: 0.1761 Acc: 93.8120%, Test Loss: 0.4283 Acc: 87.6100%\n",
      "[ 40/50 T:0:00:36.320569] Train Loss: 0.1730 Acc: 93.7760%, Test Loss: 0.4684 Acc: 87.2000%\n",
      "[ 41/50 T:0:00:36.462595] Train Loss: 0.1672 Acc: 94.0120%, Test Loss: 0.5152 Acc: 85.7200%\n",
      "[ 42/50 T:0:00:36.234521] Train Loss: 0.1621 Acc: 94.0880%, Test Loss: 0.4257 Acc: 88.0200%\n",
      "[ 43/50 T:0:00:36.147011] Train Loss: 0.1526 Acc: 94.5080%, Test Loss: 0.4606 Acc: 87.2100%\n",
      "[ 44/50 T:0:00:36.418327] Train Loss: 0.1556 Acc: 94.3900%, Test Loss: 0.3882 Acc: 88.4700%\n",
      "[ 45/50 T:0:00:36.367318] Train Loss: 0.1481 Acc: 94.6740%, Test Loss: 0.4410 Acc: 88.0700%\n",
      "[ 46/50 T:0:00:36.251328] Train Loss: 0.1421 Acc: 94.8860%, Test Loss: 0.4449 Acc: 87.5900%\n",
      "[ 47/50 T:0:00:36.157513] Train Loss: 0.1385 Acc: 95.1400%, Test Loss: 0.4138 Acc: 88.3800%\n",
      "[ 48/50 T:0:00:35.853074] Train Loss: 0.1407 Acc: 94.9760%, Test Loss: 0.3940 Acc: 88.5600%\n",
      "[ 49/50 T:0:00:35.603103] Train Loss: 0.1324 Acc: 95.3320%, Test Loss: 0.4147 Acc: 88.8600%\n",
      "[ 50/50 T:0:00:36.228017] Train Loss: 0.1250 Acc: 95.5340%, Test Loss: 0.5019 Acc: 86.8300%\n",
      "Total training time: 12:53:04.732648\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_OUTPUTS = 'outputs'\n",
    "if not os.path.exists(PATH_OUTPUTS):\n",
    "    os.mkdir(PATH_OUTPUTS)\n",
    "PATH_MODEL = os.path.join(PATH_OUTPUTS, 'resnet18_cifar10.pt')\n",
    "torch.save(model, PATH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca1e3ec0254efb300f2de71f1d983378da29e4ede80290b822db9c2199a6e419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
