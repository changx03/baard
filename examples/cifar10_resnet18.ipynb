{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_MEAN = (0.491, 0.482, 0.446)\n",
    "CIFAR10_STD = (0.247, 0.243, 0.261)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_train = tv.datasets.CIFAR10(DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "dataset_test = tv.datasets.CIFAR10(DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "loader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet18(nn.Module):\n",
    "#     def __init__(self, n_labels) -> None:\n",
    "#         super(ResNet18, self).__init__()\n",
    "#         self.n_labels = n_labels\n",
    "\n",
    "#         # Load pre-trained resnet model\n",
    "#         resnet = tv.models.resnet18(weights=tv.models.ResNet18_Weights.IMAGENET1K_V1, progress=False)\n",
    "\n",
    "#         self.conv1 = resnet.conv1\n",
    "#         self.bn1 = resnet.bn1\n",
    "#         self.relu = resnet.relu\n",
    "#         self.maxpool = resnet.maxpool\n",
    "#         self.layer1 = resnet.layer1\n",
    "#         self.layer2 = resnet.layer2\n",
    "#         self.layer3 = resnet.layer3\n",
    "#         self.layer4 = resnet.layer4\n",
    "#         self.avgpool = resnet.avgpool\n",
    "#         n_in_features = resnet.fc.in_features\n",
    "#         self.fc = nn.Linear(n_in_features, self.n_labels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# i = iter(dataset_train)\n",
    "# x, y = next(i)\n",
    "# input_size = tuple([BATCH_SIZE] + list(x.size()))\n",
    "# print('input_size:', input_size)\n",
    "\n",
    "# summary(model=ResNet18(10), input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tv.models.resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "# model = create_model()\n",
    "# summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer, device):\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    running_loss = 0.\n",
    "    corrects = 0.\n",
    "\n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    acc = corrects / n\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    running_loss = 0.\n",
    "    corrects = 0.\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x= x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            \n",
    "            preds = outputs.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / n_batches\n",
    "    acc = corrects / n\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            # print(\n",
    "            #     f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                # print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTORCH LIGHTNING CIFAR10 ~94% BASELINE TUTORIA [URL](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/cifar10-baseline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "steps_per_epoch = len(loader_train)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 0.1, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(epochs=EPOCHS):\n",
    "    early_stopping = EarlyStopping()\n",
    "\n",
    "    time_start = time.perf_counter()\n",
    "    for e in range(epochs):\n",
    "        epoch_start = time.perf_counter()\n",
    "        tr_loss, tr_acc = train(model, loader_train, loss, optimizer, device)\n",
    "        va_loss, va_acc = evaluate(model, loader_test, loss, device)\n",
    "        scheduler.step()\n",
    "        epoch_end = time.perf_counter() - epoch_start\n",
    "\n",
    "        print('[{:3d}/{:d} T:{:s}] Train Loss: {:.4f} Acc: {:.4f}%, Test Loss: {:.4f} Acc: {:.4f}%'.format(\n",
    "            e+1, epochs, str(datetime.timedelta(seconds=epoch_end)), tr_loss, tr_acc*100, va_loss, va_acc*100))\n",
    "\n",
    "        early_stopping(tr_loss)\n",
    "        if early_stopping.early_stop:\n",
    "                break\n",
    "        \n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        test_losses.append(va_loss)\n",
    "        test_accs.append(va_acc)\n",
    "    time_elapsed = time.perf_counter()\n",
    "    print('Total training time: {}'.format(str(datetime.timedelta(seconds=time_elapsed))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/50 T:0:00:27.175284] Train Loss: 1.7396 Acc: 34.9260%, Test Loss: 1.4513 Acc: 46.0400%\n",
      "[  2/50 T:0:00:26.942512] Train Loss: 1.2964 Acc: 52.6020%, Test Loss: 1.1823 Acc: 57.4800%\n",
      "[  3/50 T:0:00:26.078455] Train Loss: 1.0822 Acc: 60.9840%, Test Loss: 1.0508 Acc: 61.9800%\n",
      "[  4/50 T:0:00:26.135334] Train Loss: 0.9317 Acc: 66.6120%, Test Loss: 0.8888 Acc: 68.8500%\n",
      "[  5/50 T:0:00:26.667183] Train Loss: 0.8316 Acc: 70.4420%, Test Loss: 0.8197 Acc: 71.1000%\n",
      "[  6/50 T:0:00:25.853767] Train Loss: 0.7408 Acc: 73.7540%, Test Loss: 0.7767 Acc: 73.3700%\n",
      "[  7/50 T:0:00:25.686025] Train Loss: 0.6744 Acc: 76.3700%, Test Loss: 0.7964 Acc: 72.8600%\n",
      "[  8/50 T:0:00:25.708248] Train Loss: 0.6111 Acc: 78.4000%, Test Loss: 0.7302 Acc: 75.1600%\n",
      "[  9/50 T:0:00:26.114298] Train Loss: 0.5627 Acc: 80.4520%, Test Loss: 0.6711 Acc: 77.4500%\n",
      "[ 10/50 T:0:00:25.903536] Train Loss: 0.5249 Acc: 81.5900%, Test Loss: 0.6086 Acc: 79.3900%\n",
      "[ 11/50 T:0:00:26.000906] Train Loss: 0.4900 Acc: 82.7380%, Test Loss: 0.5724 Acc: 80.5200%\n",
      "[ 12/50 T:0:00:26.071413] Train Loss: 0.4554 Acc: 84.0180%, Test Loss: 0.5840 Acc: 80.5000%\n",
      "[ 13/50 T:0:00:26.115784] Train Loss: 0.4341 Acc: 84.6840%, Test Loss: 0.5322 Acc: 82.0700%\n",
      "[ 14/50 T:0:00:26.707572] Train Loss: 0.4036 Acc: 85.8820%, Test Loss: 0.4962 Acc: 83.2100%\n",
      "[ 15/50 T:0:00:26.772177] Train Loss: 0.3831 Acc: 86.5520%, Test Loss: 0.4926 Acc: 83.5400%\n",
      "[ 16/50 T:0:00:26.072033] Train Loss: 0.3637 Acc: 87.3000%, Test Loss: 0.4628 Acc: 84.3700%\n",
      "[ 17/50 T:0:00:26.187687] Train Loss: 0.3472 Acc: 87.8980%, Test Loss: 0.5096 Acc: 83.2700%\n",
      "[ 18/50 T:0:00:26.791881] Train Loss: 0.3260 Acc: 88.5060%, Test Loss: 0.4562 Acc: 85.0700%\n",
      "[ 19/50 T:0:00:26.498254] Train Loss: 0.3122 Acc: 89.0360%, Test Loss: 0.5399 Acc: 83.9200%\n",
      "[ 20/50 T:0:00:26.501076] Train Loss: 0.2902 Acc: 89.7280%, Test Loss: 0.4578 Acc: 85.1000%\n",
      "[ 21/50 T:0:00:26.247368] Train Loss: 0.2840 Acc: 89.9900%, Test Loss: 0.5452 Acc: 82.6400%\n",
      "[ 22/50 T:0:00:26.406222] Train Loss: 0.2628 Acc: 90.8580%, Test Loss: 0.4602 Acc: 85.4900%\n",
      "[ 23/50 T:0:00:26.196275] Train Loss: 0.2558 Acc: 91.0060%, Test Loss: 0.4505 Acc: 86.0800%\n",
      "[ 24/50 T:0:00:26.614974] Train Loss: 0.2429 Acc: 91.4860%, Test Loss: 0.5152 Acc: 84.1400%\n",
      "[ 25/50 T:0:00:26.021207] Train Loss: 0.2331 Acc: 91.7180%, Test Loss: 0.5208 Acc: 84.6200%\n",
      "[ 26/50 T:0:00:26.186764] Train Loss: 0.2281 Acc: 91.9920%, Test Loss: 0.4303 Acc: 86.3500%\n",
      "[ 27/50 T:0:00:26.408005] Train Loss: 0.2169 Acc: 92.3680%, Test Loss: 0.4591 Acc: 85.9900%\n",
      "[ 28/50 T:0:00:26.542593] Train Loss: 0.2002 Acc: 92.9300%, Test Loss: 0.5196 Acc: 84.6800%\n",
      "[ 29/50 T:0:00:26.410937] Train Loss: 0.1996 Acc: 92.9120%, Test Loss: 0.5061 Acc: 85.3600%\n",
      "[ 30/50 T:0:00:26.119995] Train Loss: 0.1936 Acc: 93.2160%, Test Loss: 0.4315 Acc: 86.9800%\n",
      "[ 31/50 T:0:00:26.445455] Train Loss: 0.1749 Acc: 93.8620%, Test Loss: 0.5123 Acc: 85.2200%\n",
      "[ 32/50 T:0:00:25.822238] Train Loss: 0.1757 Acc: 93.8200%, Test Loss: 0.5132 Acc: 85.2800%\n",
      "[ 33/50 T:0:00:26.118533] Train Loss: 0.1531 Acc: 94.6200%, Test Loss: 0.5137 Acc: 85.6000%\n",
      "[ 34/50 T:0:00:26.361374] Train Loss: 0.1602 Acc: 94.2920%, Test Loss: 0.4543 Acc: 86.7800%\n",
      "[ 35/50 T:0:00:26.277706] Train Loss: 0.1463 Acc: 94.8600%, Test Loss: 0.4997 Acc: 86.1000%\n",
      "[ 36/50 T:0:00:26.339297] Train Loss: 0.1452 Acc: 94.8320%, Test Loss: 0.4455 Acc: 87.1500%\n",
      "[ 37/50 T:0:00:26.200285] Train Loss: 0.1372 Acc: 95.1060%, Test Loss: 0.5147 Acc: 86.3900%\n",
      "[ 38/50 T:0:00:26.085527] Train Loss: 0.1324 Acc: 95.4000%, Test Loss: 0.4293 Acc: 88.0700%\n",
      "[ 39/50 T:0:00:26.195411] Train Loss: 0.1252 Acc: 95.5080%, Test Loss: 0.4819 Acc: 87.1500%\n",
      "[ 40/50 T:0:00:26.337332] Train Loss: 0.1231 Acc: 95.6260%, Test Loss: 0.4099 Acc: 88.3700%\n",
      "[ 41/50 T:0:00:25.973338] Train Loss: 0.1203 Acc: 95.7860%, Test Loss: 0.4307 Acc: 87.7400%\n",
      "[ 42/50 T:0:00:26.034471] Train Loss: 0.1106 Acc: 96.1580%, Test Loss: 0.4415 Acc: 88.0900%\n",
      "[ 43/50 T:0:00:26.062526] Train Loss: 0.1096 Acc: 96.1240%, Test Loss: 0.4321 Acc: 88.4100%\n",
      "[ 44/50 T:0:00:26.053739] Train Loss: 0.1050 Acc: 96.3020%, Test Loss: 0.5060 Acc: 86.5200%\n",
      "[ 45/50 T:0:00:26.069388] Train Loss: 0.1014 Acc: 96.3720%, Test Loss: 0.4526 Acc: 87.8700%\n",
      "[ 46/50 T:0:00:25.855774] Train Loss: 0.0977 Acc: 96.5520%, Test Loss: 0.5018 Acc: 87.4900%\n",
      "[ 47/50 T:0:00:25.764728] Train Loss: 0.0915 Acc: 96.7540%, Test Loss: 0.4548 Acc: 87.9600%\n",
      "[ 48/50 T:0:00:25.883559] Train Loss: 0.0998 Acc: 96.3900%, Test Loss: 0.5021 Acc: 87.3200%\n",
      "[ 49/50 T:0:00:26.034401] Train Loss: 0.0866 Acc: 96.9260%, Test Loss: 0.5346 Acc: 87.4200%\n",
      "[ 50/50 T:0:00:26.094502] Train Loss: 0.0762 Acc: 97.4120%, Test Loss: 0.4588 Acc: 88.0000%\n",
      "Total training time: 13:33:16.163696\n"
     ]
    }
   ],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_OUTPUTS = 'outputs'\n",
    "if not os.path.exists(PATH_OUTPUTS):\n",
    "    os.mkdir(PATH_OUTPUTS)\n",
    "PATH_MODEL = os.path.join(PATH_OUTPUTS, 'resnet18_cifar10.pt')\n",
    "torch.save(model.state_dict(), PATH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.00\n"
     ]
    }
   ],
   "source": [
    "model2 = create_model().to(device)\n",
    "model2.load_state_dict(torch.load(PATH_MODEL, map_location=device))\n",
    "va_loss, va_acc = evaluate(model2, loader_test, loss, device)\n",
    "print(f'Test accuracy: {va_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca1e3ec0254efb300f2de71f1d983378da29e4ede80290b822db9c2199a6e419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
