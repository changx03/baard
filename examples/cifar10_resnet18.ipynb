{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukec/workspace/baard_v4/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation((-7, 7)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_train = tv.datasets.CIFAR10(DATA_PATH, train=True, download=True, transform=transform_train)\n",
    "dataset_test = tv.datasets.CIFAR10(DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, n_labels) -> None:\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        # Load pre-trained resnet model\n",
    "        resnet = tv.models.resnet18(weights=tv.models.ResNet18_Weights.IMAGENET1K_V1, progress=False)\n",
    "\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "        n_in_features = resnet.fc.in_features\n",
    "        self.fc = nn.Linear(n_in_features, self.n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer, device):\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    running_loss = 0.\n",
    "    corrects = 0.\n",
    "\n",
    "    model.train()\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    acc = corrects / n\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    running_loss = 0.\n",
    "    corrects = 0.\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x= x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            \n",
    "            preds = outputs.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / n_batches\n",
    "    acc = corrects / n\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            # print(\n",
    "            #     f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                # print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.05\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(10).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "steps_per_epoch = len(loader_train)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 0.1, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=EPOCHS):\n",
    "    early_stopping = EarlyStopping()\n",
    "\n",
    "    time_start = time.perf_counter()\n",
    "    for e in range(epochs):\n",
    "        epoch_start = time.perf_counter()\n",
    "        tr_loss, tr_acc = train(model, loader_train, loss, optimizer, device)\n",
    "        va_loss, va_acc = evaluate(model, loader_test, loss, device)\n",
    "        scheduler.step()\n",
    "        epoch_end = time.perf_counter() - epoch_start\n",
    "\n",
    "        print('[{:3d}/{:d} T:{:s}] Train Loss: {:.4f} Acc: {:.4f}%, Test Loss: {:.4f} Acc: {:.4f}%'.format(\n",
    "            e+1, epochs, str(datetime.timedelta(seconds=epoch_end)), tr_loss, tr_acc*100, va_loss, va_acc*100))\n",
    "\n",
    "        early_stopping(tr_loss)\n",
    "        if early_stopping.early_stop:\n",
    "                break\n",
    "        \n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        test_losses.append(va_loss)\n",
    "        test_accs.append(va_acc)\n",
    "    time_elapsed = time.perf_counter()\n",
    "    print('Total training time: {}'.format(str(datetime.timedelta(seconds=time_elapsed))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/50 T:0:00:14.313130] Train Loss: 1.4162 Acc: 49.5360%, Test Loss: 1.0154 Acc: 65.3800%\n",
      "[  2/50 T:0:00:12.814520] Train Loss: 0.8725 Acc: 69.2320%, Test Loss: 0.7782 Acc: 73.0600%\n",
      "[  3/50 T:0:00:12.852163] Train Loss: 0.7441 Acc: 73.9980%, Test Loss: 0.6956 Acc: 76.1400%\n",
      "[  4/50 T:0:00:12.858415] Train Loss: 0.6647 Acc: 76.8160%, Test Loss: 0.6468 Acc: 77.6400%\n",
      "[  5/50 T:0:00:12.979197] Train Loss: 0.6130 Acc: 78.4380%, Test Loss: 0.6126 Acc: 79.0600%\n",
      "[  6/50 T:0:00:13.066810] Train Loss: 0.5700 Acc: 79.8220%, Test Loss: 0.5821 Acc: 79.7800%\n",
      "[  7/50 T:0:00:12.887367] Train Loss: 0.5313 Acc: 81.3800%, Test Loss: 0.5683 Acc: 80.3800%\n",
      "[  8/50 T:0:00:12.917864] Train Loss: 0.5109 Acc: 81.8940%, Test Loss: 0.5624 Acc: 80.9700%\n",
      "[  9/50 T:0:00:12.929956] Train Loss: 0.4769 Acc: 83.2360%, Test Loss: 0.5514 Acc: 81.6500%\n",
      "[ 10/50 T:0:00:12.821445] Train Loss: 0.4585 Acc: 83.7820%, Test Loss: 0.5386 Acc: 82.0200%\n",
      "[ 11/50 T:0:00:12.961934] Train Loss: 0.4331 Acc: 84.8260%, Test Loss: 0.5403 Acc: 82.1700%\n",
      "[ 12/50 T:0:00:13.021141] Train Loss: 0.4173 Acc: 85.3580%, Test Loss: 0.5454 Acc: 81.9300%\n",
      "[ 13/50 T:0:00:12.962802] Train Loss: 0.3961 Acc: 85.9840%, Test Loss: 0.5370 Acc: 82.6000%\n",
      "[ 14/50 T:0:00:13.121933] Train Loss: 0.3814 Acc: 86.3540%, Test Loss: 0.5272 Acc: 82.9100%\n",
      "[ 15/50 T:0:00:13.052813] Train Loss: 0.3690 Acc: 86.9640%, Test Loss: 0.5308 Acc: 82.7900%\n",
      "[ 16/50 T:0:00:13.079545] Train Loss: 0.3496 Acc: 87.3740%, Test Loss: 0.5399 Acc: 83.0500%\n",
      "[ 17/50 T:0:00:12.887184] Train Loss: 0.3426 Acc: 87.8680%, Test Loss: 0.5423 Acc: 82.9900%\n",
      "[ 18/50 T:0:00:13.014334] Train Loss: 0.3249 Acc: 88.4020%, Test Loss: 0.5476 Acc: 83.2000%\n",
      "[ 19/50 T:0:00:12.988881] Train Loss: 0.3123 Acc: 88.7660%, Test Loss: 0.5408 Acc: 83.4300%\n",
      "[ 20/50 T:0:00:13.071350] Train Loss: 0.2993 Acc: 89.3980%, Test Loss: 0.5489 Acc: 83.3200%\n",
      "[ 21/50 T:0:00:13.066737] Train Loss: 0.2881 Acc: 89.5560%, Test Loss: 0.5715 Acc: 82.4200%\n",
      "[ 22/50 T:0:00:12.977679] Train Loss: 0.2833 Acc: 89.9220%, Test Loss: 0.5532 Acc: 83.3100%\n",
      "[ 23/50 T:0:00:13.018133] Train Loss: 0.2627 Acc: 90.5020%, Test Loss: 0.5545 Acc: 83.7100%\n",
      "[ 24/50 T:0:00:12.528006] Train Loss: 0.2622 Acc: 90.6000%, Test Loss: 0.5588 Acc: 83.2100%\n",
      "[ 25/50 T:0:00:12.418423] Train Loss: 0.2507 Acc: 90.9900%, Test Loss: 0.5760 Acc: 83.1500%\n",
      "[ 26/50 T:0:00:12.826926] Train Loss: 0.2453 Acc: 91.2200%, Test Loss: 0.5822 Acc: 83.0300%\n",
      "[ 27/50 T:0:00:12.831298] Train Loss: 0.2330 Acc: 91.7320%, Test Loss: 0.5787 Acc: 83.5100%\n",
      "[ 28/50 T:0:00:12.951255] Train Loss: 0.2229 Acc: 92.0040%, Test Loss: 0.5915 Acc: 83.6000%\n",
      "[ 29/50 T:0:00:13.030773] Train Loss: 0.2157 Acc: 92.1800%, Test Loss: 0.5869 Acc: 83.5400%\n",
      "[ 30/50 T:0:00:12.974021] Train Loss: 0.2068 Acc: 92.5860%, Test Loss: 0.6055 Acc: 83.4000%\n",
      "[ 31/50 T:0:00:13.006678] Train Loss: 0.2020 Acc: 92.7860%, Test Loss: 0.6043 Acc: 83.5700%\n",
      "[ 32/50 T:0:00:13.029879] Train Loss: 0.1958 Acc: 92.9820%, Test Loss: 0.6096 Acc: 83.5600%\n",
      "[ 33/50 T:0:00:12.988317] Train Loss: 0.1904 Acc: 93.1920%, Test Loss: 0.6325 Acc: 83.2200%\n",
      "[ 34/50 T:0:00:12.912302] Train Loss: 0.1828 Acc: 93.5200%, Test Loss: 0.6349 Acc: 83.5000%\n",
      "[ 35/50 T:0:00:12.971651] Train Loss: 0.1780 Acc: 93.6560%, Test Loss: 0.6362 Acc: 83.7100%\n",
      "[ 36/50 T:0:00:12.984356] Train Loss: 0.1695 Acc: 93.9840%, Test Loss: 0.6362 Acc: 83.2500%\n",
      "[ 37/50 T:0:00:12.969764] Train Loss: 0.1680 Acc: 94.0960%, Test Loss: 0.6558 Acc: 82.8700%\n",
      "[ 38/50 T:0:00:12.862324] Train Loss: 0.1630 Acc: 94.2440%, Test Loss: 0.6578 Acc: 83.5300%\n",
      "[ 39/50 T:0:00:13.054067] Train Loss: 0.1593 Acc: 94.3180%, Test Loss: 0.6561 Acc: 83.5700%\n",
      "[ 40/50 T:0:00:12.985064] Train Loss: 0.1542 Acc: 94.5700%, Test Loss: 0.6562 Acc: 83.4600%\n",
      "[ 41/50 T:0:00:13.038315] Train Loss: 0.1526 Acc: 94.4940%, Test Loss: 0.6582 Acc: 83.2400%\n",
      "[ 42/50 T:0:00:13.053860] Train Loss: 0.1444 Acc: 94.8160%, Test Loss: 0.6486 Acc: 84.1000%\n",
      "[ 43/50 T:0:00:13.060461] Train Loss: 0.1474 Acc: 94.7600%, Test Loss: 0.6533 Acc: 83.6800%\n",
      "[ 44/50 T:0:00:12.963394] Train Loss: 0.1392 Acc: 95.0800%, Test Loss: 0.6248 Acc: 84.4100%\n",
      "[ 45/50 T:0:00:13.146402] Train Loss: 0.1310 Acc: 95.3180%, Test Loss: 0.6662 Acc: 83.8900%\n",
      "[ 46/50 T:0:00:13.026552] Train Loss: 0.1328 Acc: 95.2400%, Test Loss: 0.6903 Acc: 83.5900%\n",
      "[ 47/50 T:0:00:12.940382] Train Loss: 0.1301 Acc: 95.3580%, Test Loss: 0.6636 Acc: 83.9400%\n",
      "[ 48/50 T:0:00:13.024017] Train Loss: 0.1240 Acc: 95.5780%, Test Loss: 0.6895 Acc: 83.3400%\n",
      "[ 49/50 T:0:00:12.916666] Train Loss: 0.1270 Acc: 95.4500%, Test Loss: 0.6824 Acc: 83.3400%\n",
      "[ 50/50 T:0:00:12.948301] Train Loss: 0.1238 Acc: 95.6780%, Test Loss: 0.6732 Acc: 83.7700%\n",
      "Total training time: 11:42:56.333782\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca1e3ec0254efb300f2de71f1d983378da29e4ede80290b822db9c2199a6e419"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
